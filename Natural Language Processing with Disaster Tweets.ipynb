{"cells":[{"metadata":{"_uuid":"4319aaad-e9eb-4a6d-9160-d8308ea6426e","_cell_guid":"95a4410d-05e5-4e16-a15d-7c40d16797f6","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7cf4cf5-fcd8-40ff-bdca-596bbc37aac7","_cell_guid":"a7c093a9-d783-421d-9781-791b63b56a68","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\npd_train = pd.read_csv('../input/nlp-getting-started/train.csv')\npd_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be2aa6d0-869e-4c25-bccc-f4993f065fd8","_cell_guid":"2d1cb124-f523-43af-938a-58895605642d","trusted":true},"cell_type":"code","source":"X_train = pd_train.text.values\nY_train = pd_train.target.values\nX_test = pd_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19fa5028-c590-4ef3-ac89-1d6e1322f58e","_cell_guid":"87f33d5b-26c9-421e-b13d-c647ebc60cc3","trusted":true},"cell_type":"code","source":"import string\nimport nltk\ndef clean_text(line):\n    line = line.lower()\n    no_punct = [words for words in line if words not in string.punctuation]\n    line = ''.join(no_punct)\n    line = line.split()\n    line = [words for words in line if words not in nltk.corpus.stopwords.words('english')]\n    line = ' '.join(line)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6573401-5c26-4681-bb40-6a5d4daa76bb","_cell_guid":"71b4bb16-6b6a-481d-a966-40e9ea7b425f","trusted":true},"cell_type":"code","source":"import tqdm\nX_train_clean = [clean_text(line) for line in tqdm.tqdm(X_train)]\nX_test_clean = [clean_text(line) for line in tqdm.tqdm(X_test)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23fe4996-38c2-4bf6-aa23-1514cdaf5a7f","_cell_guid":"c9edaae5-4b8f-4915-9cb5-104568245856","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nvectorizer.fit(X_train_clean)\nX_train_clean_cv = vectorizer.transform(X_train_clean)\nX_test_clean_cv = vectorizer.transform(X_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2025798-e646-4460-a55f-5cea09dcffe5","_cell_guid":"91195348-16a1-49fc-9a40-bfabbe64eb9b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_cv,X_val_cv,Y_train_cv,Y_val_cv = train_test_split(X_train_clean_cv,Y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fe6c471-c3c4-451a-b330-56e700bb267f","_cell_guid":"c8ec3863-6473-4c96-b2ca-94873ed5f9b3","trusted":true},"cell_type":"code","source":"Y_train_cv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63eb61eb-8d6d-49e4-a115-0ddd2951117b","_cell_guid":"072effa0-b26d-4ed9-9bf6-903c0da99faa","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train_cv, Y_train_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27f4581a-8e14-43a3-9453-64cd7481c723","_cell_guid":"d469b41c-2cae-4886-89be-05e188202cdb","trusted":true},"cell_type":"code","source":"Y_pred_cv = clf.predict(X_val_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c48c28-cf9e-4768-8f61-14bc5ff1e63c","_cell_guid":"2fd7096c-417d-4591-a53b-be8656f4b6f4","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(Y_val_cv, Y_pred_cv))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e40f1143-4077-4b92-b13e-678b80f65928","_cell_guid":"4c3c0506-c5ef-4fb1-92b7-68a111363491","trusted":true},"cell_type":"code","source":"Y_test_pred_cv = clf.predict(X_test_clean_cv)\nid1 = pd_test['id'].values\ndf = pd.DataFrame(data={\"id\": id1, \"target\": Y_test_pred_cv})\ndf.to_csv(\"./cv.csv\", sep=',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f38bce-a765-4905-b835-77ffc08c91fd","_cell_guid":"ebd51b6d-5c10-4657-a540-277f24169ba0","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(X_train_clean)\nX_train_clean_tf = tfv.transform(X_train_clean)\nX_test_clean_tf = tfv.transform(X_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9f3f64b-bcbb-41ba-8340-effb99e0bf80","_cell_guid":"4bcc94b6-7dfe-430a-a2c9-7e6e2e73c69f","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_tf,X_val_tf,Y_train_tf,Y_val_tf = train_test_split(X_train_clean_tf,Y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18b4dbe5-6569-4d7c-b5f7-7a50465a422a","_cell_guid":"7940c289-7a00-40c8-b238-b12fe392e3fc","trusted":true},"cell_type":"code","source":"clf_tf = LogisticRegression(random_state=0).fit(X_train_tf, Y_train_tf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db16b6d3-4993-49ce-a9f6-1ae8bc23b234","_cell_guid":"8216355c-1e8d-4d34-b184-acab6b4d58cf","trusted":true},"cell_type":"code","source":"Y_pred_tf = clf_tf.predict(X_val_tf)\nprint(accuracy_score(Y_val_tf, Y_pred_tf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cec3a657-b18a-47bf-a932-093350a886eb","_cell_guid":"2e46c2f6-956d-4b30-a5a1-1f819ac2b649","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb_cv =  MultinomialNB().fit(X_train_cv, Y_train_cv)\nnb_tf =  MultinomialNB().fit(X_train_tf, Y_train_tf)\nY_pred_cv = nb_cv.predict(X_val_cv)\nY_pred_tf = nb_tf.predict(X_val_tf)\nprint(accuracy_score(Y_val_cv, Y_pred_cv))\nprint(accuracy_score(Y_val_tf, Y_pred_tf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4d32aa5-209c-40bb-8997-152616e4890b","_cell_guid":"87bd45bd-13fe-40c7-9251-e68651e252ca","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(X_train_clean)\nX_train_token = t.texts_to_sequences(X_train_clean)\nX_test_token = t.texts_to_sequences(X_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef3920b-5770-4bff-b963-3bbd5642396f","_cell_guid":"c26cdf7a-0ec2-4d84-a735-eb99cb133262","trusted":true},"cell_type":"code","source":"sent_length = 50\nX_train_token_pad = pad_sequences(X_train_token,padding='pre',maxlen=sent_length)\nX_test_token_pad = pad_sequences(X_test_token,padding='pre',maxlen=sent_length)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd90eb6c-79f8-4e14-bb36-0baecb79d364","_cell_guid":"34206174-17da-4bb5-8e98-28ad38c37ed4","trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nembedding_vector_features=100\nvoc_size = len(t.word_index)+1\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b3b481-6b5b-4fdc-ae1b-6099e516459f","_cell_guid":"bfca1cac-51ca-4720-98c2-94c2a27ad2bd","trusted":true},"cell_type":"code","source":"X_train_token_pad, X_val_token_pad, Y_train_token_pad, Y_val_token_pad = train_test_split(X_train_token_pad, Y_train, test_size=0.33, random_state=42)\nmodel.fit(X_train_token_pad,Y_train_token_pad,validation_data=(X_val_token_pad,Y_val_token_pad),epochs=10,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1a7ed8b-9674-4f77-a3e9-c7bb16f17698","_cell_guid":"b680eda0-2a13-44d2-96da-637fb4f95a01","trusted":true},"cell_type":"code","source":"Y_pred_token = model.predict_classes(X_test_token_pad)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"863cb312-967c-46a4-9704-07a0abd3aca0","_cell_guid":"c4110d9a-34c7-4d2a-947d-8c0f3bcefa56","trusted":true},"cell_type":"code","source":"id1 = pd_test['id'].values\ndf = pd.DataFrame(data={\"id\": id1, \"target\": Y_pred_token.reshape(-1)})\ndf.to_csv(\"./token.csv\", sep=',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfdfd827-931a-415c-ba4f-fcc5a01b23e0","_cell_guid":"6e7f92e0-29ac-4966-aa16-842c3b0b2454","trusted":true},"cell_type":"code","source":"def get_vocab(data):\n    vocab = {}\n    for line in tqdm.tqdm(data):\n        line = line.split()\n        for word in line:\n            try:\n                vocab[word] += 1\n            except:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eff78b81-d0a9-4920-91ef-22c9e2ee7a06","_cell_guid":"c4382f78-f027-4578-a045-5699a0950de8","trusted":true},"cell_type":"code","source":"vocab = get_vocab(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ff0350-e94a-43a2-a059-39a2d759b385","_cell_guid":"a3daf70d-bc27-4f5e-b6c2-cb371448f13e","trusted":true},"cell_type":"code","source":"import gensim.downloader as api\npath = api.load(\"word2vec-google-news-300\", return_path=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b6dd832-8959-44e9-a788-bf7459e83aa6","_cell_guid":"11165e4d-413d-49cb-b7e2-e10b1b08b2e4","trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nmodel_google = KeyedVectors.load_word2vec_format(path, binary = True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55f541b4-f534-4649-ac4d-8d0ea0530013","_cell_guid":"30c64250-c237-4cb9-a42d-3b88e86d9b76","trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(voc,model):\n    present = {}\n    not_present = {}\n    k = 0\n    i = 0\n    for word in (voc):\n        try:\n            present[word] = model[word]\n            k += voc[word]\n        except:\n            not_present[word] = voc[word]\n            i += voc[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(present) / len(voc)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(not_present.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebf7df29-3dc9-481e-a06f-b3a163338b14","_cell_guid":"6a78a22a-b34e-4b10-b4fe-bffd2774980d","trusted":true},"cell_type":"code","source":"not_present = check_coverage(vocab,model_google)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"091d0781-3a08-4a82-a7ad-e06cc77baa86","_cell_guid":"6711af81-117f-4101-8d0b-407c0882df7f","trusted":true},"cell_type":"code","source":"vocab = get_vocab(X_train_clean)\nnot_present = check_coverage(vocab,model_google)\nprint(not_present)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8056c37-01f7-40ea-bfb5-fbd5f3d2a276","_cell_guid":"b8ef3426-0b1b-4a4d-b47c-3c0afe2425d8","trusted":true},"cell_type":"code","source":"EMBEDDING_DIM =300\nword_index = t.word_index\nnum_words = len(t.word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    try:\n        embedding_vector = model_google[word]\n        embedding_matrix[i] = embedding_vector\n    except:\n        None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"856a6c69-dd1f-4674-a132-8dd82e97e17c","_cell_guid":"6f65f2c6-d851-44ba-803d-92bf679fdc0f","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten,Dropout,LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.initializers import Constant\nmax_length = 50\nmodel = Sequential()\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length,\n                            trainable=False)\n\n\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n\n\n# fit the model\nmodel1.fit(X_train_token_pad, Y_train_token_pad, batch_size=128, epochs=32, validation_data=(X_val_token_pad,Y_val_token_pad), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d2b9203-cc9c-4b5b-8fb3-0653971bbf0f","_cell_guid":"f0d9134c-1f14-4717-9ba3-d5b1c4d3c50e","trusted":true},"cell_type":"code","source":"Y_pred_token_pre = model.predict_classes(X_test_token_pad)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7d1534e-8438-4e69-b14f-4e8c38d141f8","_cell_guid":"63963ad0-f782-4004-ba31-96af81d2b48b","trusted":true},"cell_type":"code","source":"id1 = pd_test['id'].values\ndf = pd.DataFrame(data={\"id\": id1, \"target\": Y_pred_token_pre.reshape(-1)})\ndf.to_csv(\"./token_pre2.csv\", sep=',',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}